Multi‑Head Attention core:
Part3_Transformer_101/practice/attentions.py

Worked multi‑head demo:
Part3_Transformer_101/practice/01_attention_dot_qkv_multihead.py

Encoder block glue:
Part3_Transformer_101/practice/02_transformer_encoder.py (+ multi‑layer variants)

Decoder‑only (GPT‑style) checks:
Part3_Transformer_101/practice/06_transformer_decoder_only_GPT2_HG_Check.py

HuggingFace GPT‑2 utility:
Part4_NLP_101/practice/commons/gpt2.py




























References
You can watch all lecture videos and download lecture pdf files from [here](http://isoft.cnu.ac.kr/research/seminar?page=2&category=Lecture)
